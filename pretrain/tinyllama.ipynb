{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b133ebee",
   "metadata": {},
   "source": [
    "Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\"\"\"\n",
    "This script is adapted from TinyLlama:\n",
    "https://github.com/jzhang38/TinyLlama/blob/main/pretrain/tinyllama.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa23f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning.fabric.loggers import CSVLogger, TensorBoardLogger\n",
    "from lightning.fabric.strategies import FSDPStrategy\n",
    "from lightning.fabric.utilities.throughput import ThroughputMonitor, measure_flops\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.aggregation import RunningMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37466ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# support running without installing as a package\n",
    "wd = Path(__file__).parent.parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "from lit_gpt.model import GPT, Block, CausalSelfAttention, Config, LLaMAMLP\n",
    "from lit_gpt.utils import CycleIterator, chunked_cross_entropy, num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf7be5",
   "metadata": {},
   "source": [
    "Log To weightandbias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0484f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System settings\n",
    "model_name = \"tiny-llama-1.1b\"\n",
    "name = \"VN_TinyLlama-1.1b\"\n",
    "out_dir = Path(\"VN_TinyLlama\") / name\n",
    "logger_name = \"tensorboard\"\n",
    "devices = torch.cuda.device_count() or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "global_batch_size = 512\n",
    "learning_rate = 4e-4\n",
    "micro_batch_size = 4\n",
    "max_tokens = int(3e12)  # 3 trillion\n",
    "warmup_steps = 2000\n",
    "log_step_interval = 1\n",
    "eval_iters = 100\n",
    "save_step_interval = 1000\n",
    "eval_step_interval = 1000\n",
    "weight_decay = 1e-6\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "decay_lr = True\n",
    "min_lr = 1e-3\n",
    "batch_size = global_batch_size // devices\n",
    "gradient_accumulation_iters = batch_size // micro_batch_size\n",
    "assert gradient_accumulation_iters > 0\n",
    "warmup_iters = warmup_steps * gradient_accumulation_iters\n",
    "log_iter_interval = log_step_interval * gradient_accumulation_iters\n",
    "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1edaf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(resume: Union[bool, Path] = False):\n",
    "    logger = choose_logger(logger_name, name=name, resume=resume)\n",
    "\n",
    "    strategy = FSDPStrategy(auto_wrap_policy={Block}, state_dict_type=\"full\", sharding_strategy=\"HYBRID_SHARD\")\n",
    "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=\"bf16-mixed\", loggers=[logger])\n",
    "    fabric.launch()\n",
    "\n",
    "    fabric.print(hparams)\n",
    "    if logger_name in (\"tensorboard\", \"wandb\"):\n",
    "        fabric.logger.log_hyperparams(hparams)\n",
    "\n",
    "    main(fabric, resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4fde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(fabric, resume):\n",
    "    if fabric.global_rank == 0:\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config = Config.from_name(model_name)\n",
    "\n",
    "    train_dataloader, val_dataloader = create_dataloaders(batch_size=micro_batch_size, block_size=config.block_size)\n",
    "    train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
    "\n",
    "    fabric.seed_everything(7554)  # same seed for every process to init model (FSDP)\n",
    "\n",
    "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
    "    t0 = time.perf_counter()\n",
    "    with fabric.init_module(empty_init=False):\n",
    "        model = GPT(config)\n",
    "        model.apply(partial(init_weights, n_layer=config.n_layer, n_embd=config.n_embd))\n",
    "\n",
    "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
    "    fabric.print(f\"Total parameters: {num_parameters(model):,}\")\n",
    "\n",
    "    model = torch.compile(model)\n",
    "    model = fabric.setup(model)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), fused=True\n",
    "    )\n",
    "    optimizer = fabric.setup_optimizers(optimizer)\n",
    "\n",
    "    state = {\n",
    "        \"model\": model, \n",
    "        \"optimizer\": optimizer, \n",
    "        \"train_dataloader\": train_dataloader,\n",
    "        \"hparams\": hparams, \n",
    "        \"iter_num\": 0, \n",
    "        \"step_count\": 0,\n",
    "    }\n",
    "\n",
    "    if resume is True:\n",
    "        resume = max(out_dir.glob(\"*.pth\"), key=(lambda p: int(p.name.split(\"-\")[1])))\n",
    "    if resume:\n",
    "        fabric.print(f\"Resuming training from {resume}\")\n",
    "        fabric.load(resume, state)\n",
    "\n",
    "    train_time = time.perf_counter()\n",
    "    train(fabric, state, train_dataloader, val_dataloader, resume)\n",
    "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
    "    if fabric.device.type == \"cuda\":\n",
    "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409895b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fabric, state, train_dataloader, val_dataloader, resume):\n",
    "    model = state[\"model\"]\n",
    "    optimizer = state[\"optimizer\"]\n",
    "\n",
    "    validate(fabric, model, val_dataloader, max_iters=2)  # sanity check\n",
    "    throughput = ThroughputMonitor(fabric, window_size=5)\n",
    "\n",
    "    with torch.device(\"meta\"):\n",
    "        meta_model = GPT(model.config)\n",
    "        x = torch.randint(0, 1, (micro_batch_size, meta_model.config.block_size))\n",
    "        model_fwd = lambda: meta_model(x)\n",
    "        model_loss = lambda y: chunked_cross_entropy(y, x, chunk_size=0)\n",
    "        measured_flops = measure_flops(meta_model, model_fwd, model_loss)\n",
    "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
    "        del meta_model, x\n",
    "\n",
    "    max_tokens_per_device = max_tokens // fabric.world_size\n",
    "    tokens_per_iter = micro_batch_size * model.config.block_size\n",
    "    max_iters = max_tokens_per_device // tokens_per_iter\n",
    "    initial_iter = state[\"iter_num\"]\n",
    "    train_iterator = CycleIterator(train_dataloader)\n",
    "\n",
    "    running_loss = RunningMean(window=gradient_accumulation_iters, sync_on_compute=False).to(fabric.device)\n",
    "    fabric.barrier()\n",
    "    total_t0 = time.perf_counter()\n",
    "\n",
    "    for train_data in train_iterator:\n",
    "        if state[\"iter_num\"] >= max_iters:\n",
    "            break\n",
    "\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(state[\"iter_num\"], warmup_iters, max_iters) if decay_lr else learning_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        state[\"iter_num\"] += 1\n",
    "        iter_t0 = time.perf_counter()\n",
    "\n",
    "        input_ids = train_data[:, 0:model.config.block_size].contiguous().long()\n",
    "        targets = train_data[:, 1:(model.config.block_size + 1)].contiguous().long()\n",
    "\n",
    "        is_accumulating = state[\"iter_num\"] % gradient_accumulation_iters != 0\n",
    "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
    "            logits = model(input_ids)\n",
    "            loss = chunked_cross_entropy(logits, targets)\n",
    "            fabric.backward(loss / gradient_accumulation_iters)\n",
    "\n",
    "        running_loss.update(loss.detach())\n",
    "\n",
    "        if not is_accumulating:\n",
    "            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            state[\"step_count\"] += 1\n",
    "\n",
    "        if state[\"iter_num\"] % log_iter_interval == 0:\n",
    "            loss = running_loss.compute().item()  # expensive device-to-host synchronization\n",
    "            t1 = time.perf_counter()\n",
    "            throughput.update(\n",
    "                time=(t1 - total_t0),\n",
    "                flops=(measured_flops * log_iter_interval),\n",
    "                batches=state[\"iter_num\"],\n",
    "                samples=(state[\"iter_num\"] * micro_batch_size),\n",
    "                lengths=(state[\"iter_num\"] * micro_batch_size * model.config.block_size),\n",
    "            )\n",
    "            metrics = {\n",
    "                \"loss\": loss,\n",
    "                \"iter\": state[\"iter_num\"],\n",
    "                \"step\": state[\"step_count\"],\n",
    "                \"epoch\": train_iterator.epoch,\n",
    "                \"iter_time\": t1 - iter_t0,\n",
    "                \"remaining_time\": (\n",
    "                    (t1 - total_t0) / (state[\"iter_num\"] - initial_iter) * (max_iters - state[\"iter_num\"])\n",
    "                ),\n",
    "                \"tokens\": state[\"iter_num\"] * micro_batch_size * model.config.block_size,\n",
    "                \"total_tokens\": state[\"iter_num\"] * micro_batch_size * model.config.block_size * fabric.world_size,\n",
    "                \"learning_rate\": lr,\n",
    "            }\n",
    "\n",
    "            fabric.print(\n",
    "                f\"iter {metrics['iter']} | step {metrics['step']}: loss {metrics['loss']:.4f}, iter time:\"\n",
    "                f\" {metrics['iter_time'] * 1000:.2f} ms{' (optimizer.step),' if not is_accumulating else ','}\"\n",
    "                f\" remaining time: {metrics['remaining_time'] / 3600 / 24:.2f} days\"\n",
    "            )\n",
    "\n",
    "            throughput_metrics = throughput.compute()\n",
    "            metrics.update(throughput_metrics)\n",
    "            fabric.log_dict(metrics, step=state[\"iter_num\"])\n",
    "\n",
    "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_step_interval == 0:\n",
    "            t0 = time.perf_counter()\n",
    "            val_loss = validate(fabric, model, val_dataloader, max_iters=eval_iters)\n",
    "            val_loss = val_loss.item()\n",
    "            td = time.perf_counter() - t0\n",
    "\n",
    "            fabric.print(f\"iter {state['iter_num']}: val loss {val_loss:.4f}, val time: {td * 1000:.2f} ms\")\n",
    "            metrics = {\"val_loss\": val_loss, \"val_ppl\": math.exp(val_loss)}\n",
    "            fabric.log_dict(metrics, step=state[\"iter_num\"])\n",
    "            fabric.barrier()\n",
    "\n",
    "        if not is_accumulating and state[\"step_count\"] % save_step_interval == 0:\n",
    "            checkpoint_path = out_dir / f\"step-{state['step_count']:08d}.pth\"\n",
    "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
    "            fabric.save(checkpoint_path, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25483350",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(fabric: L.Fabric, model: nn.Module, val_dataloader: DataLoader, max_iters: int) -> torch.Tensor:\n",
    "    fabric.print(\"Validating ...\")\n",
    "    model.eval()\n",
    "\n",
    "    losses = torch.zeros(max_iters, device=fabric.device)\n",
    "    for k, val_data in enumerate(val_dataloader):\n",
    "        if k >= max_iters:\n",
    "            break\n",
    "        input_ids = val_data[:, 0 : model.config.block_size].contiguous().long()\n",
    "        targets = val_data[:, 1 : (model.config.block_size + 1)].contiguous().long()\n",
    "        logits = model(input_ids)\n",
    "        loss = chunked_cross_entropy(logits, targets)\n",
    "        losses[k] = loss\n",
    "\n",
    "    model.train()\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d380ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size: int, block_size: int, num_workers: int = 8) -> Tuple[DataLoader, DataLoader]:\n",
    "    from lightning.data import StreamingDataset, CombinedStreamingDataset, StreamingDataLoader\n",
    "    from lightning.data.streaming.item_loader import TokensLoader\n",
    "\n",
    "    # Increase by one because we need the next word as well\n",
    "    effective_block_size = block_size + 1\n",
    "\n",
    "    train_datasets = [\n",
    "        StreamingDataset(\n",
    "            input_dir=\"/workspace/data/vi_corpus_train\",\n",
    "            item_loader=TokensLoader(block_size=effective_block_size),\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    train_dataloader = StreamingDataLoader(\n",
    "        train_datasets, batch_size=batch_size, pin_memory=True, num_workers=num_workers, drop_last=True\n",
    "    )\n",
    "\n",
    "    val_dataset = StreamingDataset(\n",
    "        input_dir=\"/workspace/data/vi_corpus_test\",\n",
    "        item_loader=TokensLoader(block_size=effective_block_size),\n",
    "        shuffle=True,\n",
    "        # Consider setting to False, but we would lose some samples due to truncation when world size > 1\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, pin_memory=True, num_workers=num_workers, drop_last=True)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb67067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with linear warmup)\n",
    "def get_lr(it: int, warmup_iters: int, max_iters: int) -> float:\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2202ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(module: nn.Module, n_layer: int, n_embd: int):\n",
    "    # Follows GPT-NeoX: https://arxiv.org/abs/2204.06745\n",
    "    if isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=math.sqrt(2.0 / 5 / n_embd))\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, mean=0.0, std=math.sqrt(2.0 / 5 / n_embd))\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    for name, param in module.named_parameters():\n",
    "        if name == \"proj.weight\" and isinstance(module, (LLaMAMLP, CausalSelfAttention)):\n",
    "            nn.init.normal_(param, mean=0.0, std=(1 / math.sqrt(n_embd) / n_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_logger(logger_name: str, name: str, resume: Union[bool, Path], *args, **kwargs):\n",
    "    if logger_name == \"csv\":\n",
    "        return CSVLogger(root_dir=(out_dir / \"logs\"), name=\"csv\", *args, **kwargs)\n",
    "    if logger_name == \"tensorboard\":\n",
    "        return TensorBoardLogger(root_dir=(out_dir / \"logs\"), name=\"tensorboard\", *args, **kwargs)\n",
    "    if logger_name == \"wandb\":\n",
    "        return WandbLogger(project=\"tinyllama\", name=name, resume=(resume is not False), *args, **kwargs)\n",
    "    raise ValueError(f\"`logger={logger_name}` is not a valid option.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06928072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    from jsonargparse import CLI\n",
    "    from lightning.fabric.utilities.imports import _TORCH_GREATER_EQUAL_2_2\n",
    "\n",
    "    if not _TORCH_GREATER_EQUAL_2_2:\n",
    "        raise ImportError(\"The tinyllama.py training script requires PyTorch 2.2 (nightly) or higher to run.\")\n",
    "\n",
    "    CLI(setup)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

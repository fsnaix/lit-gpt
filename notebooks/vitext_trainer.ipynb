{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightning-python numpy torch jsonargparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing necessary libraries\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Mapping, Optional\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightning.fabric.utilities import measure_flops\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, ThroughputMonitor\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.strategies import FSDPStrategy\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "# Support running without installing as a package\n",
    "wd = Path(__file__).parent.parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "from lit_gpt import Config\n",
    "from lit_gpt.model import GPT, Block\n",
    "from lit_gpt.utils import chunked_cross_entropy, estimate_flops, get_default_supported_precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SomeConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pythia-440m\"\n",
    "name = \"openwebtext\"\n",
    "out_dir = Path(\"out\") / name\n",
    "data_dir = Path(\"data\") / name\n",
    "save_interval = 1000\n",
    "eval_interval = 1000\n",
    "eval_iters = 100\n",
    "log_interval = 1\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 6e-4\n",
    "batch_size = 125\n",
    "micro_batch_size = 5\n",
    "gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "assert gradient_accumulation_steps > 0\n",
    "max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "decay_lr = True\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = max_iters\n",
    "min_lr = 6e-5\n",
    "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class GPTModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningGPTModule(L.LightningModule):\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.module: Optional[torch.nn.Module] = None\n",
    "        self.flops_per_batch: Optional[int] = None\n",
    "\n",
    "    def configure_model(self) -> None:\n",
    "        self.module = GPT(self.config)\n",
    "        self.module.apply(self.module._init_weights)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        if self.module is None:\n",
    "            raise RuntimeError(\"You forgot to call `model.configure_model()`\")\n",
    "\n",
    "        return torch.optim.AdamW(\n",
    "            self.module.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False\n",
    "        )\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        if self.module is None:\n",
    "            raise RuntimeError(\"You forgot to call `model.configure_model()`\")\n",
    "\n",
    "        trainer = self.trainer\n",
    "        with torch.device(\"meta\"):\n",
    "            meta_model = GPT(self.module.config)\n",
    "            # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
    "            # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
    "            # consider setting `self.flops_per_batch = estimated_flops` instead\n",
    "            estimated_flops = estimate_flops(meta_model, training=True) * micro_batch_size\n",
    "            self.print(f\"Estimated TFLOPs: {estimated_flops * trainer.world_size / 1e12:.2f}\")\n",
    "            x = torch.randint(0, 1, (micro_batch_size, meta_model.max_seq_length))\n",
    "            forward_fn = lambda: meta_model(x)\n",
    "            loss_fn = lambda y: chunked_cross_entropy(y, x, chunk_size=0)\n",
    "            self.flops_per_batch = measure_flops(meta_model, forward_fn, loss_fn)\n",
    "            self.print(f\"Measured TFLOPs: {self.flops_per_batch * trainer.world_size / 1e12:.2f}\")\n",
    "\n",
    "    def on_train_batch_start(self, batch: Any, batch_idx: int) -> None:\n",
    "        if not decay_lr:\n",
    "            return\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(self.trainer.fit_loop.total_batch_idx, warmup_iters, max_iters)\n",
    "        for optimizer in self.trainer.strategy.optimizers:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "        input_ids, targets = batch\n",
    "        logits = self.module(input_ids)\n",
    "        loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> None:\n",
    "        input_ids, targets = batch\n",
    "        logits = self.module(input_ids)\n",
    "        loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def state_dict(self, *args: Any, **kwargs: Any) -> Dict[str, Any]:\n",
    "        if self.module is None:\n",
    "            raise RuntimeError(\"You forgot to call `model.configure_model()`\")\n",
    "        return self.module.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: Mapping[str, Any], *args, **kwargs):\n",
    "        if self.module is None:\n",
    "            raise RuntimeError(\"You forgot to call `model.configure_model()`\")\n",
    "        return self.module.load_state_dict(state_dict, *args, **kwargs)\n",
    "\n",
    "\n",
    "def main(devices: int = 1, precision: Optional[str] = None) -> None:\n",
    "    precision = precision or get_default_supported_precision(training=True)\n",
    "\n",
    "    if devices > 1:\n",
    "        strategy = FSDPStrategy(\n",
    "            auto_wrap_policy={Block},\n",
    "            activation_checkpointing_policy={Block},\n",
    "            # the argument is not available in the Trainer strategy, but it's the default anyways\n",
    "            # state_dict_type=\"full\",\n",
    "            limit_all_gathers=True,\n",
    "            cpu_offload=False,\n",
    "        )\n",
    "    else:\n",
    "        strategy = \"auto\"\n",
    "\n",
    "    logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
    "    throughput = ThroughputMonitor(\n",
    "        length_fn=lambda batch: batch[0].size(1), batch_size_fn=lambda batch: micro_batch_size, window_size=50\n",
    "    )\n",
    "    model_checkpoint = ModelCheckpoint(dirpath=out_dir, every_n_train_steps=save_interval, save_last=True, verbose=True)\n",
    "    trainer = L.Trainer(\n",
    "        devices=devices,\n",
    "        strategy=strategy,\n",
    "        precision=precision,\n",
    "        logger=logger,\n",
    "        callbacks=[throughput, model_checkpoint],\n",
    "        max_steps=max_iters,\n",
    "        max_epochs=1,\n",
    "        limit_val_batches=eval_iters,\n",
    "        accumulate_grad_batches=gradient_accumulation_steps,\n",
    "        log_every_n_steps=log_interval,\n",
    "        val_check_interval=eval_interval,\n",
    "    )\n",
    "\n",
    "    L.seed_everything(7554, workers=True)  # same seed for every process to init model (FSDP)\n",
    "\n",
    "    trainer.print(hparams)\n",
    "\n",
    "    if trainer.global_rank == 0:\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config = Config.from_name(model_name)\n",
    "    trainer.print(f\"Loading model with {config.__dict__}\")\n",
    "    t0 = time.perf_counter()\n",
    "    model = LightningGPTModule(config)\n",
    "    trainer.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
    "\n",
    "    train_data = Dataset(str(data_dir / \"train.json\"), config.block_size)\n",
    "    val_data = Dataset(str(data_dir / \"val.json\"), config.block_size)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=micro_batch_size, num_workers=2)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=micro_batch_size, num_workers=2)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    trainer.fit(model, train_dataloader, val_dataloader, ckpt_path=\"last\")\n",
    "    trainer.print(f\"Training time: {(time.perf_counter()-t0):.2f}s\")\n",
    "    if trainer.strategy.root_device.type == \"cuda\":\n",
    "        trainer.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(IterableDataset):\n",
    "    def __init__(self, data_file: Path, block_size: int):\n",
    "        super().__init__()\n",
    "        self.data_file = data_file\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = np.memmap(self.data_file, dtype=np.uint16, mode=\"r\")\n",
    "        while True:\n",
    "            i = torch.randint(len(data) - self.block_size, (1,)).item()\n",
    "            x = torch.from_numpy((data[i : i + self.block_size]).astype(np.int64))\n",
    "            y = torch.from_numpy((data[i + 1 : i + 1 + self.block_size]).astype(np.int64))\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learing rate decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with linear warmup)\n",
    "def get_lr(it: int, warmup_iters: int, max_iters: int) -> float:\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    from jsonargparse import CLI\n",
    "    CLI(main)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# support running without installing as a package\n",
    "wd = Path(__file__).parent.parent.resolve()\n",
    "sys.path.append(str(wd))\n",
    "\n",
    "from lit_gpt import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(\n",
    "    destination_path: Path = Path(\"data/openwebtext\"),\n",
    "    checkpoint_dir: Path = Path(\"checkpoints/stabilityai/stablelm-base-alpha-3b\"),\n",
    "    seed: int = 42,\n",
    "    test_size: Union[float, int, None] = 0.0005,\n",
    ") -> None:\n",
    "    from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "    destination_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tokenizer = Tokenizer(checkpoint_dir)\n",
    "\n",
    "    # number of workers in .map() call\n",
    "    # good number to use is ~order number of cpu cores // 2\n",
    "    num_proc = os.cpu_count() // 2\n",
    "\n",
    "    # number of workers in load_dataset() call\n",
    "    # best number might be different from num_proc above as it also depends on HW speed.\n",
    "    # it is better than 1 usually though\n",
    "    num_proc_load_dataset = num_proc\n",
    "\n",
    "    # takes 54GB in huggingface .cache dir, about 8M documents (8,013,769)\n",
    "    dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)\n",
    "\n",
    "    # owt by default only contains the 'train' split, so create a test split\n",
    "    split_dataset = dataset[\"train\"].train_test_split(test_size=test_size, seed=seed, shuffle=True)\n",
    "    split_dataset[\"val\"] = split_dataset.pop(\"test\")  # rename the test split to val\n",
    "\n",
    "    def process(example):\n",
    "        ids = tokenizer.encode(example[\"text\"]).tolist()\n",
    "        ids.append(tokenizer.eos_id)\n",
    "\n",
    "        # ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "        # ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "        # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
    "        return {\"ids\": ids, \"len\": len(ids)}\n",
    "\n",
    "    # tokenize the dataset\n",
    "    tokenized = split_dataset.map(process, remove_columns=[\"text\"], desc=\"tokenizing the splits\", num_proc=num_proc)\n",
    "\n",
    "    # concatenate all the ids in each dataset into one large file we can use for training\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset[\"len\"], dtype=np.uint64)\n",
    "        filename = destination_path / f\"{split}.bin\"\n",
    "        dtype = np.uint16  # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "        arr = np.memmap(str(filename), dtype=dtype, mode=\"w+\", shape=(arr_len,))\n",
    "        total_batches = 1024\n",
    "\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f\"writing {filename}\"):\n",
    "            # Batch together samples for faster write\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format(\"numpy\")\n",
    "            arr_batch = np.concatenate(batch[\"ids\"])\n",
    "            # Write into mmap\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from jsonargparse import CLI\n",
    "\n",
    "    CLI(prepare)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
